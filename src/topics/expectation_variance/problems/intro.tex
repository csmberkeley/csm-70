{\tabulinesep=1mm
\begin{tabu}{|p{16cm} |}

\hline

\textbf{Random variable}: a function $X : \omega \rightarrow R$ 
that assigns a real number to every outcome $\omega$ in the probability space.

\vspace{6mm}


\textbf{Expectation}: The expectation of a random variable $X$ is defined as
$$\E(X) = \sum_{\alpha \in A}a * \P[X = a]$$ where the sum is over all possible values taken by the random variable. Expectation is usually denoted with the symbol $\mu$.

\vspace{2mm}

\textit{Linearity of Expectation}: For any random variables $X_1, X_2, ... X_n$, expectation is linear, i.e.:
$$E(X_1 + X_2 + ... + X_n) = E(X_1) + E(X_2) + ... + E(X_n)$$
This is true even when these random variables aren't independent.

\vspace{6mm}


\textbf{Variance}: The variance of a random variable $X$ is defined as $$\var(X) = E((X - E(X))^2)\ = E(X^2) - E(X)^2$$ The latter version of variance is the one we usually use in computations.
The square root of $\var(X)$ is called the standard deviation of $X$. It is usually denoted with the variable $\sigma$. 

\\
\hline
\end{tabu}

}
