\fbox{\begin{minipage}{16.3cm}
\vspace{2 mm}
\textbf{Definition}: For a random variable $X$, expectation $\E(X)$ $=$ $$\sum_{a} a * Pr[X = A]$$
\end{minipage}}

\fbox{\begin{minipage}{16.3cm}
\vspace{2 mm}
\textbf{Definition}: For a random variable $X$ with expectation $\E(X) = \mu$, the variance of $X$ is:
\[\var(X) = \E((X - \mu)^2)\]
The square root of $\var(X)$ is called the standard deviation of $X$ and is often denoted $\sigma$. 
\end{minipage}}

\fbox{\begin{minipage}{16.3cm}
\vspace{2 mm}
\textbf{Theorem}: For a random variable $X$ with expectation $\E(X) = \mu$ and a 
constant $c$,
\[\var(X) = \E(X^2) - \mu^2\]
\[\var(cX) = c^2 * \var(X)\]
\end{minipage}}

\fbox{\begin{minipage}{16.3cm}
\vspace{2 mm}
\textbf{Theorem}: For a random variable $X$, expectation $\E(X)$ $=$ $$\sum_{a} a * Pr[X = A]$$
\vspace{2 mm}
\end{minipage}}
