{\tabulinesep=1mm
\begin{tabu}{|p{16cm} |}
\hline
P is a \textbf{transition probability matrix} if: 
\begin{enumerate}
\item All of the entries are non-negative.
\item The sum of entries in each row is 1.
\end{enumerate}

\vspace{.2cm}

A \textbf{Markov chain} is defined by four things: (\state, $\pi_0$, 
$P$, $\{X_n\}_{n=0}^{\infty}$) 
\begin{center}
\begin{tabular}{l l}
\state & Set of states \\
$\pi_0$ & Initial probability distribution (the chance you are in any of the states) \\
$P$ & Transition probability matrix \\
$\{X_n\}_{n=0}^{\infty}$ & Sequence of random variables where: \\

& \hspace{5mm} $\P[X_0 = i] = \pi_0(i), i \in \state $ \\
& \hspace{5mm} $ \P[X_{n + 1} = j | X_n = i, X_{n-1}, \dotsc , X_0] = 
\P(i, j), \forall n \geq 0, \forall i, j \in \state $
\end{tabular}
\end{center}

\vspace{.2cm}

A Markov chain is \textbf{irreducible} if we can go from any state to 
any other state, possibly in multiple steps.

\vspace{.2cm}

Periodicity has to do with the period of occurrence of a state. If a state $s$ has period 2, the Markov chain can be in $s$ at every other time point. If a state has period 1, it's aperiodic; otherwise, it's periodic. More quantitatively, define value \textbf{$d(i)$} for each state $i$ as:
\[d(i) \coloneqq g.c.d\{n > 0 | P^n(i, i) = \P[X_n = i | X_0 = i] > 0 \}, 
i \in \state\]

If $d(i) = 1$, then the Markov chain is \textbf{aperiodic}. If $d(i) 
\neq 1$, then the Markov chain is periodic and its \textbf{period} is 
$d(i)$.
\\
\hline
\end{tabu}
}



\fbox{\begin{minipage}{16.3cm}
\vspace{2mm}

A distribution $\pi$ is \textbf{invariant} for the transition probability $P$ if it satisfies the following \textbf{balance equations}
\[\pi \cdot P = \pi.\]
\textbf{Theorem 1: }A finite irreducible Markov chain has a unique 
invariant distribution.\\
\textbf{Theorem 2: }All irreducible and aperiodic Markov chains 
converge to the unique invariant distribution - i.e, after a large amount of steps, the chance you are in any given
state is given by the invariant distribution. \\
\textbf{Theorem 3: }If a Markov chain is  finite and reducible, the amount of time spent in each state approaches the invariant distribution as n grows large

Equations that model what will happen at the next step are called 
\textbf{first step equations}

\vspace{.5cm}

\begin{minipage}[b]{0.6\linewidth}
\begin{center}
\includegraphics[width=4cm]{definitions_markov_chain.jpg} 
\end{center}
\end{minipage}%
\hfill
\begin{minipage}[b]{0.4\linewidth}
Denote $\beta(i, j)$ as the expected amount of time it would take to 
move from $i$ to $j$.
$\beta(0, 1) = 1 + \frac{1}{2} \cdot \beta(0, 1)$
$\beta(1, 1) = 0$
\end{minipage}

\end{minipage}}


