\vspace{2 mm}
\textbf{Binomial Distribution}: Bin($n$, $p$)

The binomial distribution counts the number of successes when we conduct $n$ independent trials. Each trial has a probability $p$ of success. For this reason, we can think of the binomial distribution as a sum of $n$ independent Bernoulli trials, each with probability $p$. 

The probability of having $k$ successes:
$$\P[X = k] = {n \choose k} * p^k * (1 - p)^{n - k}$$

For example, if we flip a fair coin 10 times, the probability of 6 heads is

$$P(H = 6) = {10 \choose 6} \left(\frac{1}{2}\right)^6 \left(\frac{1}{2}\right)^4$$
\\

\textit{Expectation}: \\
If we were to compute the sum the traditional way, we would have to compute the sum
$$E(X) = \sum_{x \in X} x \cdot {n \choose x} p^x (1-p)^{n-x}$$

Instead of doing that, we can use the fact that the binomial distribution is the sum of $n$ independent Bernoulli distributions:
$$X = X_1 + \dotsc + X_n$$

And now use linearity of expectation:
$$E(X) = E(X_1 + \dotsc + X_n) = E(X_1) + \dotsc + E(X_n) = p + p + ... + p = np$$

\textit{Variance}: \\
We know that variance is only separable when variables are mutually independent, i.e. $var(X_1 + X_2 + ... + X_n) = var(X_1) + var(X_2) + ... + var(X_n)$ only when $X_1, X_2, ... X_n$ are mutually independent. Since our sum of Bernoulli trials is independent, we can do the following:

$$var(X) = var(X_1 + X_2 + ... + X_n) = var(X_1) + var(X_2) + ... + var(X_n)$$
$$= p(1-p) + p(1-p) + ... + p(1-p) = np(1-p)$$
